{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    Marca Modelo  Kilómetros        Precio  Edad\n",
      "0           0  Peugeot   2008       23000  21634.614423   2.0\n",
      "1           1  Peugeot   2008       17000  21153.845192   3.0\n",
      "2           2  Peugeot   2008       17500  19711.538462   3.0\n",
      "3           3  Peugeot   2008      123600  14423.076923   6.0\n",
      "4           4  Peugeot   2008       43000  12019.230769   8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# A la columna de Kilómetros le sacamos el km y lo convertimos a número\n",
    "data['Kilómetros'] = data['Kilómetros'].str.replace(' km', '').str.replace(',', '').astype(int)\n",
    "\n",
    "# Visualizar los primeros registros para asegurarnos de que se han cargado correctamente\n",
    "print(data.head())\n",
    "\n",
    "# Definir las características (features) y el objetivo (target)\n",
    "X = data[['Kilómetros', 'Edad', 'Modelo']]\n",
    "X = pd.get_dummies(X, columns=['Modelo'])\n",
    "y = data['Precio']\n",
    "\n",
    "# # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# y_train_max = y_train.max()\n",
    "# y_test = y_test / y_train.max()\n",
    "# y_train = y_train / y_train.max()\n",
    "\n",
    "# X_train = pd.get_dummies(X_train, columns=['Modelo'])\n",
    "# X_test = pd.get_dummies(X_test, columns=['Modelo'])\n",
    "\n",
    "# Normalizar las características\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir a tensores de PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninarigal/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([128, 1, 1])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/ninarigal/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([109, 1, 1])) that is different to the input size (torch.Size([109, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/ninarigal/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([124, 1, 1])) that is different to the input size (torch.Size([124, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1022276250.7428571, Test Loss: 621432307.6571429\n",
      "Epoch 2/100, Train Loss: 518525698.74285716, Test Loss: 500798869.48571426\n",
      "Epoch 3/100, Train Loss: 486070071.8857143, Test Loss: 494337278.17142856\n",
      "Epoch 4/100, Train Loss: 483001416.3428571, Test Loss: 492325050.05714285\n",
      "Epoch 5/100, Train Loss: 481405283.6571429, Test Loss: 492023018.9714286\n",
      "Epoch 6/100, Train Loss: 480779807.7714286, Test Loss: 491889964.3428571\n",
      "Epoch 7/100, Train Loss: 480448816.8, Test Loss: 490975494.4\n",
      "Epoch 8/100, Train Loss: 480953321.94285715, Test Loss: 490841393.37142855\n",
      "Epoch 9/100, Train Loss: 480178521.14285713, Test Loss: 490788041.14285713\n",
      "Epoch 10/100, Train Loss: 480586734.74285716, Test Loss: 490505046.4\n",
      "Epoch 11/100, Train Loss: 480307725.0285714, Test Loss: 490350337.82857144\n",
      "Epoch 12/100, Train Loss: 480213538.17142856, Test Loss: 490764234.51428574\n",
      "Epoch 13/100, Train Loss: 480751684.45714283, Test Loss: 490256186.9714286\n",
      "Epoch 14/100, Train Loss: 480046782.17142856, Test Loss: 490026952.6857143\n",
      "Epoch 15/100, Train Loss: 480411073.82857144, Test Loss: 489949377.37142855\n",
      "Epoch 16/100, Train Loss: 480146798.4, Test Loss: 489955435.8857143\n",
      "Epoch 17/100, Train Loss: 479784510.74285716, Test Loss: 489864864.45714283\n",
      "Epoch 18/100, Train Loss: 479836437.82857144, Test Loss: 490021479.3142857\n",
      "Epoch 19/100, Train Loss: 479811159.3142857, Test Loss: 489777878.4\n",
      "Epoch 20/100, Train Loss: 479727990.28571427, Test Loss: 490091761.37142855\n",
      "Epoch 21/100, Train Loss: 479681247.54285717, Test Loss: 489915223.3142857\n",
      "Epoch 22/100, Train Loss: 479808929.0285714, Test Loss: 489604901.94285715\n",
      "Epoch 23/100, Train Loss: 479304846.28571427, Test Loss: 489358799.0857143\n",
      "Epoch 24/100, Train Loss: 479155612.2285714, Test Loss: 489332076.8\n",
      "Epoch 25/100, Train Loss: 479712708.45714283, Test Loss: 489598802.74285716\n",
      "Epoch 26/100, Train Loss: 479149147.7714286, Test Loss: 489477302.4\n",
      "Epoch 27/100, Train Loss: 478967718.9714286, Test Loss: 489024030.62857145\n",
      "Epoch 28/100, Train Loss: 479143112.45714283, Test Loss: 489442066.28571427\n",
      "Epoch 29/100, Train Loss: 479214001.6, Test Loss: 488834718.62857145\n",
      "Epoch 30/100, Train Loss: 478935371.8857143, Test Loss: 488842709.48571426\n",
      "Epoch 31/100, Train Loss: 478857352.1142857, Test Loss: 488545200.0\n",
      "Epoch 32/100, Train Loss: 479117938.4, Test Loss: 490092380.8\n",
      "Epoch 33/100, Train Loss: 478981684.45714283, Test Loss: 488749442.74285716\n",
      "Epoch 34/100, Train Loss: 478676199.0857143, Test Loss: 489100098.28571427\n",
      "Epoch 35/100, Train Loss: 478259538.85714287, Test Loss: 488126366.17142856\n",
      "Epoch 36/100, Train Loss: 478266163.37142855, Test Loss: 488228675.6571429\n",
      "Epoch 37/100, Train Loss: 478210924.1142857, Test Loss: 488655066.9714286\n",
      "Epoch 38/100, Train Loss: 477615007.0857143, Test Loss: 487781225.6\n",
      "Epoch 39/100, Train Loss: 477775128.3428571, Test Loss: 487711386.05714285\n",
      "Epoch 40/100, Train Loss: 477767303.2, Test Loss: 487908180.1142857\n",
      "Epoch 41/100, Train Loss: 477624237.25714284, Test Loss: 488911786.05714285\n",
      "Epoch 42/100, Train Loss: 477913747.0857143, Test Loss: 487769081.14285713\n",
      "Epoch 43/100, Train Loss: 477342974.05714285, Test Loss: 487408632.2285714\n",
      "Epoch 44/100, Train Loss: 477102586.05714285, Test Loss: 487387149.25714284\n",
      "Epoch 45/100, Train Loss: 477020104.0, Test Loss: 487994061.25714284\n",
      "Epoch 46/100, Train Loss: 476902190.28571427, Test Loss: 487077405.71428573\n",
      "Epoch 47/100, Train Loss: 476614837.37142855, Test Loss: 486850285.25714284\n",
      "Epoch 48/100, Train Loss: 476780950.28571427, Test Loss: 486675572.5714286\n",
      "Epoch 49/100, Train Loss: 476275121.71428573, Test Loss: 486870249.14285713\n",
      "Epoch 50/100, Train Loss: 476281921.14285713, Test Loss: 486678347.8857143\n",
      "Epoch 51/100, Train Loss: 476306694.17142856, Test Loss: 486257897.6\n",
      "Epoch 52/100, Train Loss: 475866011.0857143, Test Loss: 486144920.6857143\n",
      "Epoch 53/100, Train Loss: 475725244.0, Test Loss: 486001427.6571429\n",
      "Epoch 54/100, Train Loss: 475629220.4, Test Loss: 486058732.8\n",
      "Epoch 55/100, Train Loss: 475884496.6857143, Test Loss: 486065554.28571427\n",
      "Epoch 56/100, Train Loss: 475257112.0, Test Loss: 485717717.94285715\n",
      "Epoch 57/100, Train Loss: 475338583.54285717, Test Loss: 485521590.4\n",
      "Epoch 58/100, Train Loss: 475560878.9714286, Test Loss: 485817543.7714286\n",
      "Epoch 59/100, Train Loss: 474875156.1142857, Test Loss: 488287786.05714285\n",
      "Epoch 60/100, Train Loss: 475851843.8857143, Test Loss: 485213534.62857145\n",
      "Epoch 61/100, Train Loss: 475051147.2, Test Loss: 485790907.8857143\n",
      "Epoch 62/100, Train Loss: 474850925.48571426, Test Loss: 485422309.0285714\n",
      "Epoch 63/100, Train Loss: 474627402.28571427, Test Loss: 484888760.2285714\n",
      "Epoch 64/100, Train Loss: 474703589.82857144, Test Loss: 485902368.0\n",
      "Epoch 65/100, Train Loss: 474466779.2, Test Loss: 484935366.4\n",
      "Epoch 66/100, Train Loss: 474740288.9142857, Test Loss: 485137309.25714284\n",
      "Epoch 67/100, Train Loss: 474345836.45714283, Test Loss: 484402591.0857143\n",
      "Epoch 68/100, Train Loss: 473959683.54285717, Test Loss: 484452775.7714286\n",
      "Epoch 69/100, Train Loss: 473568014.4, Test Loss: 484197573.0285714\n",
      "Epoch 70/100, Train Loss: 473391526.74285716, Test Loss: 484553804.8\n",
      "Epoch 71/100, Train Loss: 473553761.25714284, Test Loss: 484626720.0\n",
      "Epoch 72/100, Train Loss: 473343564.45714283, Test Loss: 484166429.71428573\n",
      "Epoch 73/100, Train Loss: 474465634.85714287, Test Loss: 483968775.3142857\n",
      "Epoch 74/100, Train Loss: 473165881.71428573, Test Loss: 485291344.0\n",
      "Epoch 75/100, Train Loss: 473272158.85714287, Test Loss: 484701179.4285714\n",
      "Epoch 76/100, Train Loss: 473239333.37142855, Test Loss: 483485601.82857144\n",
      "Epoch 77/100, Train Loss: 472821415.8857143, Test Loss: 483271151.0857143\n",
      "Epoch 78/100, Train Loss: 472603076.45714283, Test Loss: 483236974.62857145\n",
      "Epoch 79/100, Train Loss: 472563833.6, Test Loss: 483293461.0285714\n",
      "Epoch 80/100, Train Loss: 472442088.6857143, Test Loss: 483096361.14285713\n",
      "Epoch 81/100, Train Loss: 472650324.45714283, Test Loss: 483791285.48571426\n",
      "Epoch 82/100, Train Loss: 472803622.05714285, Test Loss: 483747352.2285714\n",
      "Epoch 83/100, Train Loss: 472782444.5714286, Test Loss: 482771988.5714286\n",
      "Epoch 84/100, Train Loss: 472467654.17142856, Test Loss: 482845156.1142857\n",
      "Epoch 85/100, Train Loss: 472468316.1142857, Test Loss: 482902298.9714286\n",
      "Epoch 86/100, Train Loss: 472255431.8857143, Test Loss: 482703280.45714283\n",
      "Epoch 87/100, Train Loss: 472060346.51428574, Test Loss: 482730917.0285714\n",
      "Epoch 88/100, Train Loss: 472499255.2, Test Loss: 482849029.94285715\n",
      "Epoch 89/100, Train Loss: 471734657.94285715, Test Loss: 482704964.1142857\n",
      "Epoch 90/100, Train Loss: 471978681.25714284, Test Loss: 482396222.62857145\n",
      "Epoch 91/100, Train Loss: 471610947.54285717, Test Loss: 482752832.0\n",
      "Epoch 92/100, Train Loss: 471740397.48571426, Test Loss: 482360927.54285717\n",
      "Epoch 93/100, Train Loss: 472196059.6571429, Test Loss: 482397881.6\n",
      "Epoch 94/100, Train Loss: 471633121.82857144, Test Loss: 482266282.05714285\n",
      "Epoch 95/100, Train Loss: 471325344.85714287, Test Loss: 482380289.37142855\n",
      "Epoch 96/100, Train Loss: 471358203.7714286, Test Loss: 482152717.25714284\n",
      "Epoch 97/100, Train Loss: 471367904.9142857, Test Loss: 482941485.71428573\n",
      "Epoch 98/100, Train Loss: 471160078.85714287, Test Loss: 481939077.48571426\n",
      "Epoch 99/100, Train Loss: 471644354.6857143, Test Loss: 482045059.2\n",
      "Epoch 100/100, Train Loss: 471337132.0, Test Loss: 482000654.62857145\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir el tamaño del lote\n",
    "batch_size = 128\n",
    "\n",
    "# Crear un DataLoader para los datos de entrenamiento\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Crear un DataLoader para los datos de prueba\n",
    "test_data = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # nn.Linear(3, 64),\n",
    "            nn.Linear(166, 16),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(64, 64),\n",
    "            # nn.Linear(64, 128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 64),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(64, 1)\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# Crear una instancia de la red neuronal\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entrenar la red neuronal\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Test Loss: {test_losses[-1]}\")\n",
    "\n",
    "# Visualizar la pérdida durante el entrenamiento\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 482224480.0\n",
      "Error: 482224480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninarigal/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4476, 1, 1])) that is different to the input size (torch.Size([4476, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test.to(device))\n",
    "    loss = criterion(y_pred, y_test.unsqueeze(1).to(device))\n",
    "    print(f\"Test Loss: {loss.item()}\")\n",
    "    print(f\"Error: {loss.item() }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 166x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m X_new \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_new, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_new)\n\u001b[0;32m---> 10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEl precio estimado del coche es: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m €\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 166x16)"
     ]
    }
   ],
   "source": [
    "X_new = pd.DataFrame({\n",
    "    'Kilómetros': [100000],\n",
    "    'Edad': [4],\n",
    "    'Modelo': ['SW4']\n",
    "})\n",
    "X_new = pd.get_dummies(X_new, columns=['Modelo'])\n",
    "X_new = scaler.fit_transform(X_new)\n",
    "X_new = torch.tensor(X_new, dtype=torch.float32)\n",
    "print(X_new)\n",
    "y_pred = model(X_new)\n",
    "print(f\"El precio estimado del coche es: {y_pred:.2f} €\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
